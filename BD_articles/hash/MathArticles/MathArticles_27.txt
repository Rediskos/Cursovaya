Перцептрон  ВикипедияянварьфевральмартапрельмайиюньиюльавгустсентябрьоктябрьноябрьдекабрьПерцептронПерцептронВикипедияСтатьи с нерабочими ссылкамиВикипедияСтатьи с некорректным использованием шаблонов  не указан языкСтраницы использующие волшебные ссылки ВикипедияСтатьи требующие уточнения времениВикипедияЗапросы на замену перенаправлений переводамиВикипедияИзбранные статьи о компьютерахВикипедияИзбранные статьи по математикеВикипедияИзбранные статьи по алфавитуПерцептронАналоговые компьютерыПерцептронрусскийПерцептронМатериал из Википедии  свободной энциклопедииЭто стабильная версия отпатрулированная  декабря СостояниеотпатрулированаПерейти к навигацииПерейти к поиску Логическая схема перцептрона с тремя выходамиПерцептрон или персептрон  англ от лат восприятие нем математическая или компьютерная модель восприятия информации мозгом кибернетическая модель мозга предложенная Фрэнком Розенблаттом в  году и впервые реализованная в виде электронной машины Марк  в  году Перцептрон стал одной из первых моделей нейросетей а Марк первым в мире нейрокомпьютеромПерцептрон состоит из трёх типов элементов а именно поступающие от датчиков сигналы передаются ассоциативным элементам а затем реагирующим элементам Таким образом перцептроны позволяют создать набор ассоциаций между входными стимулами и необходимой реакцией на выходе В биологическом плане это соответствует преобразованию например зрительной информации в физиологический ответ от двигательных нейронов Согласно современной терминологии перцептроны могут быть классифицированы как искусственные нейронные сетис одним скрытым слоем с пороговой передаточной функциейс прямым распространением сигналаНа фоне роста популярности нейронных сетей в  году вышла книга Марвина Минского и Сеймура Паперта которая показала принципиальные ограничения перцептронов Это привело к смещению интереса исследователей искусственного интеллекта в противоположную от нейросетей область символьных вычислений  Кроме того изза сложности математического исследования перцептронов а также отсутствия общепринятой терминологии возникли различные неточности и заблужденияВпоследствии интерес к нейросетям и в частности работам Розенблатта возобновился Так например сейчас стремительно развивается биокомпьютинг который в своей теоретической основе вычислений в том числе базируется на нейронных сетях а перцептрон воспроизводят на основе бактериородопсинсодержащих плёнокСодержание Появление перцептрона Описание элементарного перцептрона Основные понятия теории перцептронов Описание на основе сигналов Описание на основе предикатов Историческая классификация Алгоритмы обучения Обучение с учителем Обучение без учителя Метод обратного распространения ошибки Традиционные заблуждения Терминологические неточности Функциональные заблуждения Задача  Обучаемость линейно неразделимым задачам Обучаемость на малом числе примеров Стабилизация весов и сходимость Экспоненциальный рост числа скрытых элементов Возможности и ограничения модели Возможности модели Ограничения модели Применение перцептронов Прогнозирование и распознавание образов Управление агентами См также Примечания Источники Литература СсылкиПоявление перцептронаправить  править код Схема искусственного нейрона базового элемента любой нейронной сетиВ  году в своей статье Логическое исчисление идей относящихся к нервной активности Уоррен МакКаллок и Уолтер Питтс предложили понятие искусственной нейронной сети В частности ими была предложена модель искусственного нейрона Дональд Хебб в работе Организация поведения  года описал основные принципы обучения нейроновЭти идеи несколько лет спустя развил американский нейрофизиолог Фрэнк Розенблатт Он предложил схему устройства моделирующего процесс человеческого восприятия и назвал его перцептроном Перцептрон передавал сигналы от фотоэлементов представляющих собой сенсорное поле в блоки электромеханических ячеек памяти Эти ячейки соединялись между собой случайным образом в соответствии с принципами коннективизма В  году в Корнеллской Лаборатории Аэронавтики было успешно завершено моделирование работы перцептрона на компьютере   а два года спустя  июня  года в Корнеллском университете был продемонстрирован первый нейрокомпьютер Марк который был способен распознавать некоторые буквы английского алфавита Фрэнк Розенблатт со своим творением МаркЧтобы научить перцептрон классифицировать образы был разработан специальный итерационный метод обучения проб и ошибок напоминающий процесс обучения человека метод коррекции ошибки Кроме того при распознании той или иной буквы перцептрон мог выделять характерные особенности буквы статистически чаще встречающиеся чем малозначимые отличия в индивидуальных случаях Тем самым перцептрон был способен обобщать буквы написанные различным образом почерком в один обобщённый образ Однако возможности перцептрона были ограниченными машина не могла надёжно распознавать частично закрытые буквы а также буквы иного размера расположенные со сдвигом или поворотом нежели те которые использовались на этапе её обученияОтчёт по первым результатам появился ещё в  году тогда Розенблаттом была опубликована статья Перцептрон Вероятностная модель хранения и организации информации в головном мозге Но подробнее свои теории и предположения относительно процессов восприятия и перцептронов он описывает  году в книге Принципы нейродинамики Перцептроны и теория механизмов мозга В книге он рассматривает не только уже готовые модели перцептрона с одним скрытым слоем но и многослойных перцептронов с перекрёстными третья глава и обратными четвёртая глава связями В книге также вводится ряд важных идей и теорем например доказывается теорема сходимости перцептронаОписание элементарного перцептронаправить  править код Поступление сигналов с сенсорного поля в решающие блоки элементарного перцептрона в его физическом воплощении Логическая схема элементарного перцептрона Веса  связей могут иметь значения   или  то есть отсутствие связи Веса  связей  могут быть любымиЭлементарный перцептрон состоит из элементов трёх типов элементов элементов и одного элемента элементы это слой сенсоров или рецепторов В физическом воплощении они соответствуют например светочувствительным клеткам сетчатки глаза или фоторезисторам матрицы камеры Каждый рецептор может находиться в одном из двух состояний покоя или возбуждения и только в последнем случае он передаёт единичный сигнал в следующий слой ассоциативным элементамэлементы называются ассоциативными потому что каждому такому элементу как правило соответствует целый набор ассоциация элементов элемент активизируется как только количество сигналов от элементов на его входе превысило некоторую величину   Таким образом если набор соответствующих элементов располагается на сенсорном поле в форме буквы Д элемент активизируется если достаточное количество рецепторов сообщило о появлении белого пятна света в их окрестности то есть элемент будет как бы ассоциирован с наличиемотсутствием буквы Д в некоторой областиСигналы от возбудившихся элементов в свою очередь передаются в сумматор  причём сигнал от го ассоциативного элемента передаётся с коэффициентом   Этот коэффициент называется весом  связиТак же как и элементы элемент подсчитывает сумму значений входных сигналов помноженных на веса линейную форму элемент а вместе с ним и элементарный перцептрон выдаёт  если линейная форма превышает порог  иначе на выходе будет  Математически функцию реализуемую элементом можно записать так     Обучение элементарного перцептрона состоит в изменении весовых коэффициентов   связей  Веса связей  которые могут принимать значения    и значения порогов элементов выбираются случайным образом в самом начале и затем не изменяются Описание алгоритма см нижеПосле обучения перцептрон готов работать в режиме распознавания или обобщения В этом режиме перцептрону предъявляются ранее неизвестные ему объекты и перцептрон должен установить к какому классу они принадлежат Работа перцептрона состоит в следующем при предъявлении объекта возбудившиеся элементы передают сигнал элементу равный сумме соответствующих коэффициентов   Если эта сумма положительна то принимается решение что данный объект принадлежит к первому классу а если она отрицательна то ко второмуОсновные понятия теории перцептроновправить  править кодСерьёзное ознакомление с теорией перцептронов требует знания базовых определений и теорем совокупность которых и представляет собой базовую основу для всех последующих видов искусственных нейронных сетей Но как минимум необходимо понимание хотя бы с точки зрения теории сигналов являющееся оригинальным то есть описанное автором перцептрона Ф РозенблаттомОписание на основе сигналовправить  править код Пороговая функция реализуемая простыми  и элементами Пороговая функция реализуемая простым элементомДля начала определим составные элементы перцептрона которые являются частными случаями искусственного нейрона с пороговой передаточной функциейПростым элементом сенсорным является чувствительный элемент который от воздействия какоголибо из видов энергии например света звука давления тепла итп вырабатывает сигнал Если входной сигнал превышает некоторый порог  на выходе элемента получаем  в противном случае Простым элементом ассоциативным называется логический решающий элемент который даёт выходной сигнал  когда алгебраическая сумма его входных сигналов превышает некоторую пороговую величину  говорят что элемент активный в противном случае выход равен нулюПростым элементом реагирующим то есть действующим называется элемент который выдаёт сигнал  если сумма его входных сигналов является строго положительной и сигнал  если сумма его входных сигналов является строго отрицательной Если сумма входных сигналов равна нулю выход считается либо равным нулю либо неопределённымЕсли на выходе любого элемента мы получаем  то говорят что элемент активен или возбуждёнВсе рассмотренные элементы называются простыми так как они реализуют скачкообразные функции Розенблатт утверждал также что для решения более сложных задач могут потребоваться другие виды функций например линейнаяВ результате Розенблатт ввёл следующие определенияПерцептрон представляет собой сеть состоящую из   элементов с переменной матрицей взаимодействия  элементы которой   весовые коэффициенты определяемой последовательностью прошлых состояний активности сетиПерцептроном с последовательными связями называется система в которой все связи начинающиеся от элементов с логическим расстоянием  от ближайшего элемента оканчиваются на элементах с логическим расстоянием  от ближайшего элементаПростым перцептроном называется любая система удовлетворяющая следующим пяти условиямв системе имеется только один элемент естественно он связан всеми элементамисистема представляет собой перцептрон с последовательными связями идущими только от элементов к элементам и от элементов к элементамвеса всех связей от элементов к элементам  связей неизменнывремя передачи каждой связи равно либо нулю либо фиксированной постоянной   все активирующие функции   элементов имеют вид   где   алгебраическая сумма всех сигналов поступающих одновременно на вход элемента  Элементарным перцептроном называется простой перцептрон у которого все элементы простые В этом случае его активизирующая функция имеет вид   Дополнительно можно указать на следующие концепции предложенные в книге и позднее развитые в рамках теории нейронных сетейПерцептрон с перекрёстными связями это система в которой существуют связи между элементами одного типа   или  находящиеся на одинаковом логическом расстоянии от элементов причём все остальные связи последовательного типаПерцептрон с обратной связью это система в которой существует хотя бы одна связь от логически более удалённого элемента к менее удалённому Согласно современной терминологии такие сети называются рекуррентнымиПерцептрон с переменными  связями это система в которой снято ограничение на фиксированные связи от элементов к элементам Доказано что путём оптимизации  связей можно добиться значительного улучшения характеристик перцептронаОписание на основе предикатовправить  править кодОсновная статья Перцептрон предикатное описаниеМарвин Минский изучал свойства параллельных вычислений частным случаем которых на то время был перцептрон Для анализа его свойств ему пришлось переизложить теорию перцептронов на язык предикатов Суть подхода заключалась в следующем множеству сигналов от элементов была сопоставлена переменная каждому элементу был сопоставлен предикат  фи от икс названный частным предикатомкаждому элементу был сопоставлен предикат  пси зависящий от частных предикатовнаконец перцептроном было названо устройство способное вычислять все предикаты типа Применительно к зрительному перцептрону переменная  символизировала образ какойлибо геометрической фигуры стимул Частный предикат позволял распознавать каждый свою фигуру Предикат  означал ситуацию когда линейная комбинация        коэффициенты передачи превышала некоторый порог Учёные выделили  семейств перцептронов обладающих по их мнению интересными свойствамиПерцептроны ограниченные по диаметру каждая фигура  распознаваемая частными предикатами не превосходит по диаметру некоторую фиксированную величинуПерцептроны ограниченного порядка каждый частный предикат зависит от ограниченного количества точек из Перцептроны Гамбы каждый частный предикат должен быть линейной пороговой функцией то есть миниперцептрономСлучайные перцептроны перцептроны ограниченного порядка где частные предикаты представляют собой случайно выбранные булевы функции В книге отмечается что именно эта модель наиболее подробно изучалась группой РозенблаттаОграниченные перцептроны множество частных предикатов бесконечно а множество возможных значений коэффициентов   конечноХотя такой математический аппарат позволил применить анализ только к элементарному перцептрону Розенблатта он вскрыл много принципиальных ограничений для параллельных вычислений от которых не свободен ни один вид современных искусственных нейронных сетейИсторическая классификацияправить  править код Архитектура многослойного перцептрона обоих подтиповПонятие перцептрона имеет интересную но незавидную историю В результате неразвитой терминологии нейронных сетей прошлых лет резкой критики и непонимания задач исследования перцептронов а иногда и ложного освещения прессой изначальный смысл этого понятия исказился Сравнивая разработки Розенблатта и современные обзоры и статьи можно выделить  довольно обособленных класса перцептроновПерцептрон с одним скрытым слоемЭто классический перцептрон которому посвящена большая часть книги Розенблатта и рассматриваемый в данной статье у него имеется по одному слою   и элементовОднослойный перцептронЭто модель в которой входные элементы напрямую соединены с выходными с помощью системы весов Является простейшей сетью прямого распространения линейным классификатором и частным случаем классического перцептрона в котором каждый элемент однозначно соответствует одному элементу  связи имеют вес  и все элементы имеют порог    Однослойные перцептроны фактически являются формальными нейронами то есть пороговыми элементами МакКаллока Питтса Они имеют множество ограничений в частности они не могут идентифицировать ситуацию когда на их входы поданы разные сигналы задача  см нижеМногослойный перцептрон по РозенблаттуЭто перцептрон в котором присутствуют дополнительные слои элементов Его анализ провёл Розенблатт в третьей части своей книгиМногослойный перцептрон по РумельхартуЭто перцептрон в котором присутствуют дополнительные слои элементов причём обучение такой сети проводится по методу обратного распространения ошибки и обучаемыми являются все слои перцептрона в том числе  Является частным случаем многослойного перцептрона РозенблаттаВ настоящее время в литературе под термином перцептрон понимается чаще всего однослойный перцептрон англ  причём существует распространённое заблуждение что именно этот простейший тип моделей предложил Розенблатт В противоположность однослойному ставят многослойный перцептрон англ  опять же чаще всего подразумевая многослойный перцептрон Румельхарта а не Розенблатта Классический перцептрон в такой дихотомии относят к многослойнымАлгоритмы обученияправить  править кодВажным свойством любой нейронной сети является способность к обучению Процесс обучения является процедурой настройки весов и порогов с целью уменьшения разности между желаемыми целевыми и получаемыми векторами на выходе В своей книге Розенблатт пытался классифицировать различные алгоритмы обучения перцептрона называя их системами подкрепленияСистема подкрепления это любой набор правил на основании которых можно изменять с течением времени матрицу взаимодействия или состояние памяти перцептронаОписывая эти системы подкрепления и уточняя возможные их виды Розенблатт основывался на идеях Д Хебба об обучении предложенных им в  году которые можно перефразировать в следующее правило состоящее из двух частейЕсли два нейрона по обе стороны синапса соединения активизируются одновременно то есть синхронно то прочность этого соединения возрастаетЕсли два нейрона по обе стороны синапса активизируются асинхронно то такой синапс ослабляется или вообще отмираетОбучение с учителемправить  править кодОсновная статья Метод коррекции ошибкиКлассический метод обучения перцептрона это метод коррекции ошибки Он представляет собой такой вид обучения с учителем при котором вес связи не изменяется до тех пор пока текущая реакция перцептрона остаётся правильной При появлении неправильной реакции вес изменяется на единицу а знак  определяется противоположным от знака ошибкиДопустим мы хотим обучить перцептрон разделять два класса объектов так чтобы при предъявлении объектов первого класса выход перцептрона был положителен  а при предъявлении объектов второго класса отрицательным  Для этого выполним следующий алгоритмСлучайным образом выбираем пороги для элементов и устанавливаем связи  далее они изменяться не будутНачальные коэффициенты   полагаем равными нулюПредъявляем обучающую выборку объекты например круги либо квадраты с указанием класса к которым они принадлежатПоказываем перцептрону объект первого класса При этом некоторые элементы возбудятся Коэффициенты   соответствующие этим возбуждённым элементам увеличиваем на Предъявляем объект второго класса и коэффициенты   тех элементов которые возбудятся при этом показе уменьшаем на Обе части шага  выполним для всей обучающей выборки В результате обучения сформируются значения весов связей  Теорема сходимости перцептрона описанная и доказанная Ф Розенблаттом с участием Блока Джозефа Кестена и других исследователей работавших вместе с ним показывает что элементарный перцептрон обучаемый по такому алгоритму независимо от начального состояния весовых коэффициентов и последовательности появления стимулов всегда приведёт к достижению решения за конечный промежуток времениОбучение без учителяправить  править кодКроме классического метода обучения перцептрона Розенблатт также ввёл понятие об обучении без учителя предложив следующий способ обученияАльфасистема подкрепления это система подкрепления при которой веса всех активных связей   которые ведут к элементу   изменяются на одинаковую величину  а веса неактивных связей за это время не изменяютсяЗатем с разработкой понятия многослойного перцептрона альфасистема была модифицирована и её стали называть дельтаправило Модификация была проведена с целью сделать функцию обучения дифференцируемой например сигмоидной что в свою очередь нужно для применения метода градиентного спуска благодаря которому возможно обучение более одного слояМетод обратного распространения ошибкиправить  править кодОсновная статья Алгоритм обратного распространения ошибкиДля обучения многослойных сетей рядом учёных в том числе Д Румельхартом был предложен градиентный алгоритм обучения с учителем проводящий сигнал ошибки вычисленный выходами перцептрона к его входам слой за слоем Сейчас это самый популярный метод обучения многослойных перцептронов Его преимущество в том что он может обучить все слои нейронной сети и его легко просчитать локально Однако этот метод является очень долгим к тому же для его применения нужно чтобы передаточная функция нейронов была дифференцируемой При этом в перцептронах пришлось отказаться от бинарного сигнала и пользоваться на входе непрерывными значениямиТрадиционные заблужденияправить  править кодВ результате популяризации искусственных нейронных сетей журналистами и маркетологами был допущен ряд неточностей которые при недостаточном изучении оригинальных работ по этой тематике неверно истолковывались молодыми на то время учёными В результате по сей день можно встретиться с недостаточно глубокой трактовкой функциональных возможностей перцептрона по сравнению с другими нейронными сетями разработанными в последующие годыкогдаТерминологические неточностиправить  править кодСамая распространённая ошибка связанная с терминологией это определение перцептрона как нейронной сети без скрытых слоёв однослойного перцептрона см выше Эта ошибка связана с недостаточно проработанной терминологией в области нейросетей на раннем этапе их разработки Ф Уоссерменом была сделана попытка определённым образом классифицировать различные виды нейронных сетей Началоцитаты    Началоцитаты    Началоцитаты    Началоцитаты   Началоцитаты   Началоцитаты  Как видно из публикаций нет общепринятого способа подсчёта числа слоёв в сети Многослойная сеть состоит из чередующихся множеств нейронов и весов Входной слой не выполняет суммирования Эти нейроны служат лишь в качестве разветвлений для первого множества весов и не влияют на вычислительные возможности сети По этой причине первый слой не принимается во внимание при подсчёте слоев и сеть считается двухслойной так как только два слоя выполняют вычисления Далее веса слоя считаются связанными со следующими за ними нейронами Следовательно слой состоит из множества весов со следующими за ними нейронами суммирующими взвешенные сигналы Конеццитаты   В результате такого представления перцептрон попал под определение однослойная нейронная сеть Отчасти это верно потому что у него нет скрытых слоёв обучающихся нейронов веса которых адаптируются к задаче И поэтому всю совокупность фиксированных связей системы из  к элементам можно логически заменить набором модифицированных по жёсткому правилу новых входных сигналов поступающих сразу на Аэлементы устранив тем самым вообще первый слой связей Но тут как раз не учитывают что такая модификация превращает нелинейное представление задачи в линейноеПоэтому просто игнорирование не обучаемых слоёв с фиксированными связями в элементарном перцептроне это  связи позволяет делать неправильные выводы о возможностях нейросети Так Минский поступил очень корректно переформулировав Аэлемент как предикат то есть функцию наоборот Уоссермен уже потерял такое представление и у него Аэлемент просто вход почти эквивалентный элементу При такой терминологической путанице упускается из виду тот факт что в перцептроне происходит отображение рецептивного поля элементов на ассоциативное поле Аэлементов в результате чего и происходит преобразование любой линейно неразделимой задачи в линейно разделимуюФункциональные заблужденияправить  править код Решение элементарным перцептроном задачи  Порог всех элементов   Большинство функциональных заблуждений сводятся к якобы невозможности решения перцептроном линейно неразделимой задачи Но вариаций на эту тему достаточно много рассмотрим главные из нихЗадача править  править кодЗаблуждение перцептрон не способен решить задачу Очень распространённое и самое несерьёзное заявление На изображении справа показано решение этой задачи перцептроном Данное заблуждение возникает вопервых изза того что неправильно интерпретируют определение перцептрона данного Минским см выше а именно предикаты сразу приравнивают входам хотя предикат у Минского это функция идентифицирующая целый набор входных значений  Вовторых изза того что классический перцептрон Розенблатта путают с однослойным перцептроном изза терминологической неточности описанной вышеСледует обратить особое внимание на то что однослойный перцептрон в современной терминологии и однослойный перцептрон в терминологии Уоссермана разные объекты И объект изображённый на иллюстрации в терминологии Уоссермана есть двухслойный перцептронОбучаемость линейно неразделимым задачамправить  править кодЗаблуждение выбором случайных весов можно достигнуть обучения и линейно неразделимым вообще любым задачам но только если повезёт и в новых переменных выходах нейронов задача окажется линейно разделимой Но может и не повезтиТеорема сходимости перцептрона доказывает что нет и не может быть никакого может и не повезти при равенстве Аэлементов числу стимулов и не особенной матрице вероятность решения равна  То есть при отображении рецепторного поля на ассоциативное поле большей на одну размерности случайным нелинейным оператором нелинейная задача превращается в линейно разделимую А следующий обучаемый слой уже находит линейное решение в другом пространстве входовНапример обучение перцептрона для решения задачи  см на иллюстрации проводится следующими этапамиВесаИтерацииВходные сигналы           Обучаемость на малом числе примеровправить  править кодЗаблуждение если в задаче размерность входов довольно высока а обучающих примеров мало то в таком слабо заполненном пространстве число удач может и не оказаться малым Это свидетельствует лишь о частном случае пригодности перцептрона а не его универсальностиДанный аргумент легко проверить на тестовой задаче под названием шахматная доска или губка с водой Дана цепочка из  единиц или нулей параллельно поступающих на входы перцептрона Если эта цепочка является зеркально симметричной относительно центра то на выходе  иначе  Обучающие примеры  все это важно   цепочекМогут быть вариации данной задачи напримерВозьмём чёрнобелое изображение размером  элементов пикселей Входными данными для перцептрона будут координаты точки  бит   бит итого нужно  элементов на выходе потребуем цвет точки Обучаем перцептрон всем точкам всему изображению В итоге имеем  различных пар стимулреакция Обучить без ошибокЕсли данный аргумент справедлив то перцептрон не сможет ни при каких условиях обучиться не делая ни одной ошибки Иначе перцептрон не ошибётся ни разуНа практике оказывается что данная задача очень проста для перцептрона чтобы её решить перцептрону достаточно  Аэлементов вместо полных   необходимых для любой задачи При этом число итераций порядка  При  Аэлементов перцептрон не сходится за   итераций Если же увеличить число Аэлементов до   то схождения можно ожидать за  итерацийТакой аргумент появляется изза того что данную задачу путают с задачей Минского о предикате чётностьСтабилизация весов и сходимостьправить  править кодЗаблуждение в перцептроне Розенблатта столько Аэлементов сколько входов И сходимость по Розенблатту это стабилизация весовУ Розенблатта читаемЕсли число стимулов в пространстве  равно    то есть больше числа Аэлементов элементарного перцептрона то существует некоторая классификация С для которой решения не существуетОтсюда следует чтоу Розенблатта число Аэлементов равно числу стимулов обучающих примеров а не числу входовсходимость по Розенблатту это не стабилизация весов а наличие всех требуемых классификаций то есть по сути отсутствие ошибокЭкспоненциальный рост числа скрытых элементовправить  править кодЗаблуждение если весовые коэффициенты к элементам скрытого слоя Аэлементам фиксированы то необходимо чтобы количество элементов скрытого слоя либо их сложность экспоненциально возрастало с ростом размерности задачи числа рецепторов Тем самым теряется их основное преимущество способность решать задачи произвольной сложности при помощи простых элементовРозенблаттом было показано что число Аэлементов зависит только от числа стимулов которые нужно распознать см предыдущий пункт или теорему сходимости перцептрона Таким образом при возрастании числа рецепторов если количество Аэлементов фиксировано непосредственно не зависит возможность перцептрона к решению задач произвольной сложностиТакое заблуждение происходит от следующей фразы МинскогоПри исследовании предиката чётность мы видели что коэффициенты могут расти с ростом  числа точек на изображении экспоненциальноКроме того Минский исследовал и другие предикаты например равенство Но все эти предикаты представляют собой достаточно специфическую задачу на обобщение а не на распознавание или прогнозирование Так например чтобы перцептрон мог выполнять предикат четность он должен сказать чётно или нет число чёрных точек на чёрнобелом изображении а для выполнения предиката равенство сказать равна ли правая часть изображения левой Ясно что такие задачи выходят за рамки задач распознавания и прогнозирования и представляют собой задачи на обобщение или просто на подсчёт определённых характеристик Это и было убедительно показано Минским и является ограничением не только перцептронов но и всех параллельных алгоритмов которые не способны быстрее последовательных алгоритмов вычислить такие предикатыПоэтому такие задачи ограничивают возможности всех нейронных сетей и перцептронов в частности но это никак не связанно с фиксированными связями первого слоя так как вопервых речь шла о величине коэффициентов связей второго слоя а вовторых вопрос только в эффективности а не принципиальной возможности То есть перцептрон можно обучить и этой задаче но требуемые для этого ёмкость памяти и скорость обучения будут больше чем при применении простого последовательного алгоритма Введение же обучаемых весовых коэффициентов в первом слое лишь ухудшит положение дел ибо потребует большего времени обучения потому что переменные связи между  и  скорее препятствуют чем способствуют процессу обучения Причём при подготовке перцептрона к задаче распознавания стимулов особого типа для сохранения эффективности потребуются особые условия стохастического обучения что было показано Розенблаттом в экспериментах с перцептроном с переменными  связямиВозможности и ограничения моделиправить  править кодОсновная статья Возможности и ограничения перцептроновВозможности моделиправить  править код Пример классификации объектов Зелёная линия граница классовСам Розенблатт рассматривал перцептрон прежде всего как следующий важный шаг в сторону исследования и использования нейронных сетей а не как оконченный вариант машины способной мыслить  Ещё в предисловии к своей книге он отвечая на критику отмечал что программа по исследованию перцептрона связана главным образом не с изобретением устройств обладающих искусственным интеллектом а с изучением физических структур и нейродинамических принциповРозенблатт предложил ряд психологических тестов для определения возможностей нейросетей эксперименты по различению обобщению по распознаванию последовательностей образованию абстрактных понятий формированию и свойствам самосознания творческого воображения и другие Некоторые из этих экспериментов далеки от современных возможностей перцептронов поэтому их развитие происходит больше философски в пределах направления коннективизма Тем не менее для перцептронов установлены два важных факта находящие применение в практических задачах возможность классификации объектов и возможность аппроксимации границ классов и функцийВажным свойством перцептронов является их способность к обучению причём по довольно простому и эффективному алгоритму см вышеОграничения моделиправить  править кодОсновная статья Перцептроны книга Некоторые задачи которые перцептрон не способен решить   преобразования группы переносов  из какого количества частей состоит фигура  внутри какого объекта нет другой фигуры  какая фигура внутри объектов повторяется два раза    задачи на определение связности фигурСам Розенблатт выделил два фундаментальных ограничения для трёхслойных перцептронов состоящих из одного слоя одного слоя и слоя отсутствие у них способности к обобщению своих характеристик на новые стимулы или новые ситуации а также неспособность анализировать сложные ситуации во внешней среде путём расчленения их на более простыеВ  году Марвин Минский и Сеймур Паперт опубликовали книгу Перцептроны где математически показали что перцептроны подобные розенблаттовским принципиально не в состоянии выполнять многие из тех функций которые хотели получить от перцептронов К тому же в то время была слабо развита теория о параллельных вычислениях а перцептрон полностью соответствовал принципам таких вычислений По большому счёту Минский показал преимущество последовательных вычислений перед параллельным в определённых классах задач связанных с инвариантным представлением Его критику можно разделить на три темыПерцептроны имеют ограничения в задачах связанных с инвариантным представлением образов то есть независимым от их положения на сенсорном поле и относительно других фигур Такие задачи возникают например если нам требуется построить машину для чтения печатных букв или цифр так чтобы эта машина могла распознавать их независимо от положения на странице то есть чтобы на решение машины не оказывали влияния перенос поворот растяжениесжатие символов или если нам нужно определить из скольких частей состоит фигура или находятся ли две фигуры рядом или нет Минским было доказано что этот тип задач невозможно полноценно решить с помощью параллельных вычислений в том числе перцептронаПерцептроны не имеют функционального преимущества над аналитическими методами например статистическими в задачах связанных с прогнозированием Тем не менее в некоторых случаях они представляют более простой и производительный метод анализа данныхБыло показано что некоторые задачи в принципе могут быть решены перцептроном но могут потребовать нереально большого времени или нереально большой памятиКнига Минского и Паперта существенно повлияла на пути развития науки об искусственном интеллекте так как переместила научный интерес и субсидии правительственных организаций США на другое направление исследований символьный подход в ИИПрименение перцептроновправить  править кодЗдесь будут показаны только основы практического применения перцептрона на двух различных задачах Задача прогнозирования и эквивалентная ей задача распознавания образов требует высокой точности а задача управления агентами высокой скорости обучения Поэтому рассматривая эти задачи можно полноценно ознакомиться с возможностями перцептрона однако этим далеко не исчерпываются варианты его использованияВ практических задачах от перцептрона потребуется возможность выбора более чем из двух вариантов а значит на выходе у него должно находиться более одного элемента Как показано Розенблаттом характеристики таких систем не отличаются существенно от характеристик элементарного перцептронаПрогнозирование и распознавание образовправить  править код Взаимодействие обучающегося агента со средой Важной частью такой системы являются обратные связиВ этих задачах от перцептрона требуется установить принадлежность объекта к какомулибо классу по его параметрам например по внешнему виду форме силуэту Причём точность распознавания будет во многом зависеть от представления выходных реакций перцептрона Здесь возможны три типа кодирования конфигурационное позиционное и гибридное Позиционное кодирование когда каждому классу соответствует свой элемент даёт более точные результаты чем другие виды Такой тип использован например в работе Э Куссуль и др Перцептроны Розенблатта для распознавания рукописных цифр Однако оно неприменимо в тех случаях когда число классов значительно например несколько сотен В таких случаях можно применять гибридное конфигурационнопозиционное кодирование как это было сделано в работе С Яковлева Система распознавания движущихся объектов на базе искусственных нейронных сетейУправление агентамиправить  править кодВ искусственном интеллекте часто рассматриваются обучающиеся адаптирующиеся к окружающей среде агенты При этом в условиях неопределённости становится важным анализировать не только текущую информацию но и общий контекст ситуации в которую попал агент поэтому здесь применяются перцептроны с обратной связью Кроме того в некоторых задачах становится важным повышение скорости обучения перцептрона например с помощью моделирования рефрактерностиПосле периода известного как Зима искусственного интеллекта интерес к кибернетическим моделям возродился в х годах так как сторонники символьного подхода в ИИ так и не смогли подобраться к решению вопросов о Понимании и Значении изза чего машинный перевод и техническое распознавание образов до сих пор обладает неустранимыми недостатками Сам Минский публично выразил сожаление что его выступление нанесло урон концепции перцептронов хотя книга лишь показывала недостатки отдельно взятого устройства и некоторых его вариаций Но в основном ИИ стал синонимом символьного подхода который выражался в составлении все более сложных программ для компьютеров моделирующих сложную деятельность человеческого мозгаСм такжеправить  править кодСвёрточная нейронная сетьБайесовская сетьКогнитронИстория искусственного интеллектаСтереотипПримечанияправить  править код Вариант перцептрон изначальный используется в переводе книги Розенблатта  также в справочнике Толковый словарь по искусственному интеллекту  Авторысоставители АНАверкин МГГаазеРапопорт ДАПоспелов М Радио и связь  с Вариант персептрон встречается чаще он возник при переводе книги Минского и Пейперта  см также Энциклопедия кибернетики Том  МихЯч Киев Гл изд УСЭ  С Архивная копия от  марта  на   Марк в частности был системой имитирующей человеческий глаз и его взаимодействие с мозгом Трёхслойные по классификации принятой у Розенблатта и двухслойные по современной системе обозначений с той особенностью что первый слой не обучаемый К символьному подходу относятся например создание экспертных систем организация баз знаний анализ текстов Формально элементы как и элементы представляют собой сумматоры с порогом то есть одиночные нейроны Изложение в этом разделе несколько упрощено изза сложности анализа на основе предикатов Предикат эквивалентен входу лишь в частном случае только когда он зависит от одного аргумента ММБонгард считает эту задачу наисложнейшей для проведения гиперплоскости в пространстве рецепторов На первых этапах развития науки об искусственном интеллекте её задача рассматривалась в абстрактном смысле создание систем напоминающих по разуму человека см искусственный общий интеллект Современные формулировки задач в ИИ как правило более аккуратныИсточникиправить  править код        Логическое исчисление идей относящихся к нервной активности                     Т    С                 Современное издание                         Появление перцептронанедоступная ссылка   Системы распознавания образовнеопрнедоступная ссылка Дата обращения  октября  Архивировано декабря года   Минский М Пейперт С с              неопрнедоступная ссылка Дата обращения  мая  Архивировано февраля года     Розенблатт Ф с  Фомин С В Беркинблит М Б Математические проблемы в биологии Розенблатт Ф с  Розенблатт Ф с  Брюхомицкий Ю А Нейросетевые модели для систем информационной безопасности     Розенблатт Ф с     Розенблатт Ф с      Розенблатт Ф с  Розенблатт Ф с    Розенблатт Ф с  Розенблатт Ф с  Минский Пейперт с  Минский Пейперт с  Розенблатт Ф с  Хайкин С  с  Розенблатт Ф с  Хайкин С  с   Уоссермен Ф Нейрокомпьютерная техника Теория и практика  Бонгард М М с  Минский М Пейперт С с  Розенблатт Ф с  Минский Пейперт с   недословно упрощённо для выразительности Розенблатт стр  Розенблатт стр  Розенблатт Ф с  Розенблатт Ф с  см Ежов А А Шумский С А Нейрокомпьютинг  Лекция  Обучение с учителем Распознавание образов Минский М Пейперт С с  Минский М Пейперт С с  Минский М Пейперт С с  Минский Пейперт с  Минский Пейперт с  Розенблатт Ф с  Яковлев С С Использование принципа рекуррентности Джордана в перцептроне Розенблатта Журнал АВТОМАТИКА И ВЫЧИСЛИТЕЛЬНАЯ ТЕХНИКА Рига     Яковлев С С                       Исследование принципа рефрактерности в рекуррентных нейронных сетях перевод Литератураправить  править кодБонгард М М Проблема узнавания М Наука  с Архивная копия от  июня  на  Брюхомицкий Ю А Нейросетевые модели для систем информационной безопасности Учебное пособие Таганрог Издво ТРТУ  снедоступная ссылкаМакКаллок У С Питтс В Логическое исчисление идей относящихся к нервной активности           Автоматы Сб М  С  Архивировано июня годаМинский М Пейперт С Персептроны   М Мир  с Архивная копия от  июня  на  Розенблатт Ф Принципы нейродинамики Перцептроны и теория механизмов мозга            М Мир  с Архивная копия от  мая  на  Уоссермен Ф Нейрокомпьютерная техника Теория и практика       М Мир  с   Архивная копия от  июня  на  Хайкин С Нейронные сети Полный курс       е изд М Вильямс  с  Яковлев С С Система распознавания движущихся объектов на базе искусственных нейронных сетей ИТК НАНБ Минск  С         Перцептроны Розенблатта для распознавания рукописных цифр         С    Архивировано августа годаангл          Использование перцептрона для выделения сайтов инициации в                    С  англСсылкиправить  править кодПерцептроннеопр     Дата обращения  января  Архивировано августа годаПоявление перцептронанеопрнедоступная ссылка Дата обращения  января  Архивировано апреля годаЕжов А А Шумский С А Нейрокомпьютинг и его применения в экономике и бизнесенеопр  ИНТУИТ  Дата обращения  января Редько В Г Искусственные нейронные сетинеопр  Дата обращения  января Яковлев С С Линейность и инвариантность в искусственных нейронных сетяхнеопр недоступная ссылка  Дата обращения  января  Архивировано августа года         англ  Дата обращения  января  Архивировано августа годаБеркинблит М Б Нейронные сети Глава Перцептроны и другие обучающиеся классификационные системынеопрнедоступная ссылка  Дата обращения  января  Архивировано августа годаТипы искусственных нейронных сетейСеть прямого распространения Сеть радиальнобазисных функцийОднослойный перцептронМногослойный перцептрон Розенблата  РумельхартаСеть ХопфилдаЦепь МарковаМашина БольцманаОграниченная машина БольцманаАвтокодировщик Шумоподавляющий автокодировщик  Разреженный автокодировщик  Вариационный автокодировщикГлубокая сеть доверияСвёрточная нейронная сетьГлубинная свёрточная нейронная сетьРазвёртывающая нейронная сетьГлубинная свёрточная обратная графическая сетьГенеративносостязательная сетьРекуррентная нейронная сетьРекурсивные нейронные сетиДолгая краткосрочная памятьУправляемый рекуррентный блокНейронные машины ТьюрингаДвунаправленная сеть Двунаправленная рекуррентная нейросеть  Двунаправленная сеть с долгой краткосрочной памятью  Двунаправленные управляемые рекуррентные нейроныГлубинная остаточная сетьНейронная эхосетьМетод экстремального обученияМетод неустойчивых состоянийМетод опорных векторовСеть КохоненаСамоорганизующаяся карта КохоненаКапсульная нейронная сетьАссоциативная память на нейронных сетяхМашинное обучение и  ЗадачиЗадача классификацииОбучение без учителяОбучение с частичным привлечением учителяРегрессионный анализАссоциативные правилаВыделение признаковОбучение признакамОбучение ранжированиюГрамматический выводОнлайновое обучениеОбучение с учителемМетод ближайших соседейНаивный байесовский классификаторДерево решенийМетод опорных векторовЛинейная регрессияЛогистическая регрессияПерцептронАнсамбли моделейБэггингБустинг Метод релевантных векторовКластерный анализМетод среднихМетод нечёткой кластеризацииИерархическая кластеризацияалгоритмСнижение размерностиФакторный анализМетод главных компонентНеотрицательное матричное разложениеСтруктурное прогнозированиеГрафовая вероятностная модельБайесовская сетьСкрытая марковская модельВыявление аномалийМетод ближайших соседейЛокальный уровень выбросаГрафовые вероятностные моделиБайесовская сетьМарковская сетьСкрытая марковская модельНейронные сетиОграниченная машина БольцманаСамоорганизующаяся картаФункция активацииСигмоидаРадиальнобазисная функцияМетод обратного распространения ошибкиГлубокое обучениеМногослойный перцептронРекуррентная нейронная сетьДолгая краткосрочная памятьУправляемый рекуррентный блокСвёрточная нейронная сетьАвтокодировщикОбучение с подкреплениемМарковский процессУравнение БеллманаЖадный алгоритм  ТеорияТеория Вапника  ЧервоненкисаДилемма смещениядисперсииТеория вычислительного обученияМинимизация эмпирического рискаОккамово обучение Статистическая теория обученияЖурналы и конференцииЭта статья входит в число избранных статей русскоязычного раздела ВикипедииИсточник  ПерцептронКатегории ПерцептронАналоговые компьютерыСкрытые категории ВикипедияСтатьи с нерабочими ссылкамиВикипедияСтатьи с некорректным использованием шаблонов  не указан языкСтраницы использующие волшебные ссылки ВикипедияСтатьи требующие уточнения времениВикипедияЗапросы на замену перенаправлений переводамиВикипедияИзбранные статьи о компьютерахВикипедияИзбранные статьи по математикеВикипедияИзбранные статьи по алфавитуНавигацияПерсональные инструментыВы не представились системеОбсуждениеВкладСоздать учётную записьВойтиПространства имёнСтатьяОбсуждениеВариантыПросмотрыЧитатьПравитьПравить кодИсторияЕщёПоиск    Навигация    Заглавная страницаРубрикацияУказатель АЯИзбранные статьиСлучайная статьяТекущие события    Участие    Сообщить обошибкеСообществоФорумСвежие правкиНовые страницыСправкаПожертвовать    Инструменты    Ссылки сюдаСвязанные правкиСлужебные страницыПостоянная ссылкаСведения остраницеЭлемент ВикиданныхЦитировать страницу    Печатьэкспорт    Создать книгуСкачать как Версия для печати    На других языках     МакедонскиСрпски  УкранськаПравить ссылки Эта страница в последний раз была отредактирована  декабря  в Текст доступен по лицензии    в отдельных случаях могут действовать дополнительные условияПодробнее см Условия использования зарегистрированный товарный знак некоммерческой организации   Политика конфиденциальностиОписание ВикипедииОтказ от ответственностиСвяжитесь с намиРазработчикиСтатистикаЗаявление о кукиМобильная версия                   ШаблонПримечания          ШаблонСтатья          Шаблон           ШаблонИзбраннаястатья           ШаблонНейросети            Шаблон            ШаблонБсокр            ШаблонКогда           ШаблонКнига                 